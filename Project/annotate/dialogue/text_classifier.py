import datetime
import os
import re
from typing import Dict, Any

import kashgari
import keras
import tqdm
from kashgari.embeddings import BareEmbedding, BERTEmbedding, TransformerEmbedding
from kashgari.layers import L, AttentionWeightedAverageLayer
from kashgari.tasks.classification.base_model import BaseClassificationModel
from kashgari.tokenizer import BertTokenizer
from sklearn.model_selection import train_test_split
from tensorflow import keras

file_path = "./data/data.txt"

#é¢„è®­ç»ƒæ¨¡å‹çš„è®¿é—®è·¯å¾„
pretrained_model_path = "./pretrained_model/"
bert_model_path = {"bert": pretrained_model_path + "bert-base-chinese",
                   "albert": pretrained_model_path + "albert_base_zh_additional_36k_steps",
                   "albert_large": pretrained_model_path + "albert_large",
                   "albert_xxlarge": pretrained_model_path + "albert_xxlarge",
                   "baidu": pretrained_model_path + "baidu_ernie",
                   "xunfei": pretrained_model_path + "chinese_roberta_wwm_large_ext_L-24_H-1024_A-16",
                   "roberta": pretrained_model_path + "RoBERTa-large-clue"}

model_folder = './pretrained_model/albert_large'
checkpoint_path = os.path.join(model_folder, 'model.ckpt-best')
config_path = os.path.join(model_folder, 'albert_config.json')
vocab_path = os.path.join(model_folder, 'vocab_chinese.txt')

models = []

bert_version = 1
bert_name = "roberta"
bert_path = bert_model_path[bert_name]

exclude = False
exclude_label = ["A", "B"]

#ä¸€æ¬¡è®­ç»ƒæ‰€é€‰å–çš„æ ·æœ¬æ•°
batch_size = 32
#ä¸€ä¸ªepochæ˜¯æŒ‡æŠŠæ‰€æœ‰è®­ç»ƒæ•°æ®å®Œæ•´çš„è¿‡ä¸€é
#epochå’Œepochä¹‹é—´ï¼Œè®­ç»ƒæ•°æ®ä¼šè¢«shuffle
epochs = 5

# æœ‰å‡ ä¸ªå·ç§¯æ ¸å°±ä¼šäº§ç”Ÿå‡ ä¸ªğ‘“ğ‘’ğ‘ğ‘¡â„ğ‘’ğ‘Ÿğ‘šğ‘ğ‘ã€‚è¾“å…¥æ•°æ®ç»è¿‡å·ç§¯æ“ä½œè¾“å‡ºğ‘“ğ‘’ğ‘ğ‘¡â„ğ‘’ğ‘Ÿğ‘šğ‘ğ‘ï¼Œä»åå­—ä¹Ÿå¯ä»¥çŸ¥é“ï¼ˆç¿»è¯‘å°±æ˜¯ç‰¹å¾å›¾ï¼‰è¿™ä¸ªæ˜¯ç»è¿‡å·ç§¯æ ¸æå–åå¾—åˆ°çš„ç‰¹å¾ã€‚
#
# å¤šä¸ªğ‘“ğ‘’ğ‘ğ‘¡â„ğ‘’ğ‘Ÿğ‘šğ‘ğ‘å°±æ„å‘³ç€æˆ‘ä»¬æå–äº†å¤šä¸ªç‰¹å¾å€¼ï¼Œè¿™æ ·æˆ–è®¸å°±å¯ä»¥æ›´åŠ å‡†ç¡®åœ°è¯†åˆ«æ•°æ®ã€‚
class ClassificationModel(BaseClassificationModel):

    @classmethod
    def get_default_hyper_parameters(cls) -> Dict[str, Dict[str, Any]]:
        return {
            'spatial_dropout': {
                'rate': 0.2
            },
            'bilstm_0': {
                'units': 64,
                'return_sequences': True
            },
            'conv_0': {
                # å·ç§¯æ ¸åˆåè¿‡æ»¤å™¨(ğ‘“ğ‘–ğ‘™ğ‘¡ğ‘’ğ‘Ÿ)ã€‚
                # æ¯ä¸ªå·ç§¯æ ¸æœ‰ä¸‰ä¸ªå±æ€§ï¼šé•¿å®½æ·±ï¼Œè¿™é‡Œä¸€èˆ¬æ·±åº¦ä¸éœ€è¦è‡ªå·±å®šä¹‰ï¼Œæ·±åº¦æ˜¯å’Œè¾“å…¥çš„æ•°æ®æ·±åº¦ç›¸åŒï¼›
                'filters': 32,
                #å·ç§¯æ ¸çš„å¤§å°æ˜¯5*5*1
                'kernel_size': 5,
                'kernel_initializer': 'normal',
                'padding': 'valid',
                'activation': 'relu',
                'strides': 1
            },
            #å¼€å§‹çš„å·ç§¯æ ¸çš„å€¼æ˜¯éšæœºçš„ï¼Œä¹‹åæ¯æ¬¡çš„å‘åè®¡ç®—çš„è¿‡ç¨‹ä¸­ä¼šå¾—å‡ºè¿™ä¸ªå›¾åƒçš„ç±»åˆ«ï¼Œå½“ç„¶è¿™ä¸ªç¬¬ä¸€æ¬¡çš„ç»“æœå¤§éƒ¨åˆ†éƒ½æ˜¯ä¸å‡†ç¡®çš„ï¼Œä¹‹åç»è¿‡ğ‘™ğ‘œğ‘ ğ‘ ğ‘“ğ‘¢ğ‘›ğ‘ğ‘¡ğ‘–ğ‘œğ‘›çš„ä½œç”¨ï¼ŒCNNä¼šæ›´æ–°è¿™äº›å·ç§¯æ ¸ä¸­çš„å€¼ï¼Œç„¶åå†æ¥ä¸€æ¬¡å­¦ä¹ ã€‚è¿™æ ·ç»è¿‡å¤šæ¬¡çš„å­¦ä¹ ï¼ŒCNNå°±ä¼šæ‰¾åˆ°å·ç§¯æ ¸çš„æœ€ä½³å‚æ•°ï¼Œä½¿å¾—æå–çš„ç‰¹å¾èƒ½å‡†ç¡®åŒºåˆ†è¿™äº›å›¾ç‰‡ï¼Œè¿™æ ·ä¹Ÿå°±å®Œæˆäº†CNNçš„å­¦ä¹ è¿‡ç¨‹ã€‚
        'maxpool': {},
            'attn': {},
            'average': {},
            'concat': {
                'axis': 1
            },
            'dropout': {
                'rate': 0.5
            },
            'activation_layer': {
                'activation': 'softmax'
            },
        }

    def build_model_arc(self):
        output_dim = len(self.processor.label2idx)
        config = self.hyper_parameters
        embed_model = self.embedding.embed_model

        layers_rcnn_seq = []
        # layers_rcnn_seq.append(L.SpatialDropout1D(**config['spatial_dropout']))
        layers_rcnn_seq.append(L.Bidirectional(L.LSTM(**config['bilstm_0'])))
        layers_rcnn_seq.append(L.Conv1D(**config['conv_0']))

        layers_sensor = []
        layers_sensor.append(L.GlobalMaxPooling1D())
        layers_sensor.append(AttentionWeightedAverageLayer())
        layers_sensor.append(L.GlobalAveragePooling1D())
        layer_concat = L.Concatenate(**config['concat'])

        layers_full_connect = []
        layers_full_connect.append(L.Dropout(**config['dropout']))
        layers_full_connect.append(L.Dense(output_dim, **config['activation_layer']))

        tensor = embed_model.output
        for layer in layers_rcnn_seq:
            tensor = layer(tensor)

        tensors_sensor = [layer(tensor) for layer in layers_sensor]
        tensor_output = layer_concat(tensors_sensor)
        # tensor_output = L.concatenate(tensor_sensors, **config['concat'])

        for layer in layers_full_connect:
            tensor_output = layer(tensor_output)

        self.tf_model = keras.Model(embed_model.inputs, tensor_output)


def load_data(filepath, bertpath, is_exclude, exclude_label_list):
    x_train, x_valid, x_test, y_train, y_valid, y_test = [], [], [], [], [], []
    content_list, label_list = [], []
    label_counter = {}

    global embed, tokenizer
    if bert_version == -1:
        embed = BareEmbedding(embedding_size=128, processor=kashgari.CLASSIFICATION, sequence_length=50)

    if bert_version == 1:
        embed = BERTEmbedding(bertpath, task=kashgari.CLASSIFICATION)
        tokenizer = embed.tokenizer
        embed.processor.add_bos_eos = False

    if bert_version == 2:
        tokenizer = BertTokenizer.load_from_vocab_file(vocab_path)
        embed = TransformerEmbedding(vocab_path, config_path, checkpoint_path,
                                     bert_type='albert', task=kashgari.CLASSIFICATION)

    pattern = '[ï¼Œã€ã€‚:ï¼šï¼›/ï¼ˆï¼‰()ã€Šã€‹â€œâ€"ï¼Ÿ,.;?Â·â€¦0-9A-Za-z+=-]'
    lines = open(filepath, 'r', encoding='utf-8').read().splitlines()
    for line in tqdm.tqdm(lines):
        rows = line.split('\t')
        if len(rows) == 4:
            content = tokenizer.tokenize(re.sub(pattern, "", rows[0]))
            label = rows[1]
            if is_exclude:
                if label not in exclude_label_list:
                    x_train.append(content)
                    y_train.append(label)
                    if label not in label_list:
                        label_list.append(label)
                        label_counter[label] = 1
                    else:
                        label_counter[label] += 1
            else:
                #æ¥åˆ°è¿™é‡Œ
                x_train.append(content)
                y_train.append(label)
                if label not in label_list:
                    label_list.append(label)
                    label_counter[label] = 1
                else:
                    label_counter[label] += 1

    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=0)
    print("=== Data Summary ===")
    print("Train\t", len(x_train))
    print("Test\t", len(x_test))
    print("Label\t", len(label_list))
    for key, value in label_counter.items():
        print(key + '\t', value)
    return x_train, y_train, x_test, y_test, embed


if __name__ == '__main__':
    print("=== Data Loading ===")
    train_x, train_y, test_x, test_y, bert_embedding = load_data(file_path, bert_path, exclude, exclude_label)

    print("=== Model Loading ===")

    models.append(("CNN-BiLSTM-Attention", ClassificationModel(bert_embedding)))

    model_name = "saved_model"

    if len(models) != 0:
        print(len(models), "Model.")
        for model_ in models:
            print(model_[0])

        for model_ in models:
            print("=== Model Training ===")
            model_name = model_[0]
            model = model_[1]

            print("Pretrained Model:", bert_name)
            print("Classification Model:", model_name)
            print("Batch Size: {}, Epochs: {}".format(str(batch_size), str(epochs)))
            model.fit(train_x, train_y, test_x, test_y, batch_size, epochs)

            print("=== Model Evaluating ===")
            model.evaluate(test_x, test_y)
            report = model.evaluate(test_x, test_y, output_dict=True)
            # print("Precision:", precision_score(y_data, y_pred, average="weighted"))
            # print("Recall:", recall_score(y_data, y_pred, average="weighted"))
            # print("F1:", f1_score(y_data, y_pred, average="weighted"))

            print("=== Model Saving ===")
            datatime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
            file_prefix = bert_name + "_" + model_name + "_" + str(batch_size) + "_" + str(epochs) + "_" + datatime
            pred_record_name = "./pred_record/" + file_prefix + ".txt"
            model.save('./saved_model/' + file_prefix)
            with open(pred_record_name, 'a+') as f:
                f.write(str(report))
            # with open(pred_record_name, 'a+') as f:
            #     for i in range(len(y_data)):
            #         f.write(y_data[i] + '\t' + y_pred[i] + '\n')
            print(file_prefix + " Done!")
            # plot_model(model, to_file='./saved_model/' + file_prefix + datatime + '/model.png')

    else:
        loaded_model = kashgari.utils.load_model('saved_model/' + model_name)
        print(test_x)
        print(test_y)
        print(loaded_model.predict_top_k_class(test_x))
