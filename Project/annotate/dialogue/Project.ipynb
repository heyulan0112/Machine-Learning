{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdXjmnvzYxJY",
        "outputId": "52301158-508b-4601-d9ec-3e5061391ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3x_sCXo_Mdp"
      },
      "outputs": [],
      "source": [
        "!pip install kashgari\n",
        "!pip install tensorflow_addons==0.13.0\n",
        "!pip install tensorflow==2.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "820MwChd8Dut",
        "outputId": "ce8b34f1-9a90-4039-b309-df3a80611e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-15 21:57:07,683 [DEBUG] kashgari - ------------------------------------------------\n",
            "2022-05-15 21:57:07,685 [DEBUG] kashgari - Loaded transformer model's vocab\n",
            "2022-05-15 21:57:07,687 [DEBUG] kashgari - config_path       : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/bert_config.json\n",
            "2022-05-15 21:57:07,690 [DEBUG] kashgari - vocab_path      : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/vocab.txt\n",
            "2022-05-15 21:57:07,693 [DEBUG] kashgari - checkpoint_path : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/bert_model.ckpt\n",
            "2022-05-15 21:57:07,695 [DEBUG] kashgari - Top 50 words    : ['[PAD]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]']\n",
            "2022-05-15 21:57:07,698 [DEBUG] kashgari - ------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 19550/19550 [00:00<00:00, 22771.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Data Summary:\n",
            "Train\t 13096\n",
            "Test\t 6451\n",
            "Label\t 9\n",
            "9\t 1095\n",
            "1\t 6426\n",
            "7\t 1185\n",
            "2\t 3493\n",
            "3\t 3854\n",
            "4\t 1667\n",
            "8\t 226\n",
            "6\t 1129\n",
            "5\t 472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import datetime\n",
        "import os\n",
        "import re\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import kashgari\n",
        "from kashgari.tokenizers import BertTokenizer\n",
        "from kashgari.embeddings import BareEmbedding, BertEmbedding\n",
        "from kashgari.layers import L\n",
        "from kashgari.tasks.classification.abc_model import ABCClassificationModel\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras.backend as K\n",
        "from typing import Dict, Any\n",
        "import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "pre_trained = 'roberta'\n",
        "file_path = \"/content/drive/MyDrive/ColabNotebooks/project_data/data.txt\"\n",
        "bert_path = \"/content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/\"\n",
        "\n",
        "models = []\n",
        "\n",
        "def load_data(filepath, bertpath):\n",
        "  X, y = [], []\n",
        "  x_train, x_test, y_train, y_test = [], [], [], []\n",
        "  label_list = []\n",
        "  label_counter = {}\n",
        "\n",
        "  global embed, tokenizer\n",
        "  # embed = BareEmbedding(embedding_size=128, processor=kashgari.processors.class_processor, sequence_length=50)\n",
        "  embed = BertEmbedding(bertpath)\n",
        "  tokenizer = BertTokenizer.load_from_vocab_file(os.path.join(bert_path, 'vocab.txt'))\n",
        "\n",
        "  pattern = '[，、。:：；/（）()《》“”\"？,.;?·…0-9A-Za-z+=-]'\n",
        "  lines = open(filepath, 'r', encoding='utf-8').read().splitlines()\n",
        "  for line in tqdm.tqdm(lines): \n",
        "    rows = line.split('\\t')\n",
        "    if len(rows) == 4:\n",
        "      content = tokenizer.tokenize(re.sub(pattern, \"\", rows[0]))\n",
        "      label = rows[1]\n",
        "      X.append(content)\n",
        "      y.append(label)\n",
        "      if label not in label_list:\n",
        "        label_list.append(label)\n",
        "        label_counter[label] = 1\n",
        "      else:\n",
        "        label_counter[label] += 1\n",
        "  x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.33,shuffle=True)\n",
        "\n",
        "  print(\"Project Data Summary:\")\n",
        "  print(\"Train\\t\", len(x_train))\n",
        "  print(\"Test\\t\", len(x_test))\n",
        "  print(\"Label\\t\", len(label_list))\n",
        "  for key, value in label_counter.items():\n",
        "    print(key + '\\t', value)\n",
        "  return x_train, y_train, x_test, y_test,embed\n",
        "\n",
        "Xtr, ytr, Xts, yts, bert_embedding = load_data(file_path, bert_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mwyqpFNLJ6sZ"
      },
      "outputs": [],
      "source": [
        "class My_CNN_Model(ABCClassificationModel):\n",
        "    @classmethod\n",
        "    def default_hyper_parameters(cls) -> Dict[str, Dict[str, Any]]:\n",
        "        return {\n",
        "            'conv_0': {\n",
        "                'filters': 32,\n",
        "                'kernel_size': 5,\n",
        "                'kernel_initializer': 'normal',\n",
        "                'padding': 'valid',\n",
        "                'activation': 'relu',\n",
        "                'strides': 1\n",
        "            },\n",
        "            'concat': {\n",
        "                'axis': 1\n",
        "            },\n",
        "            'dropout': {\n",
        "                'rate': 0.5\n",
        "            },\n",
        "            'activation_layer': {\n",
        "                'activation': 'softmax'\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def build_model_arc(self):\n",
        "        output_dim = len(self.label_processor.vocab2idx)\n",
        "        config = self.hyper_parameters\n",
        "        embed_model = self.embedding.embed_model\n",
        "\n",
        "        # 1D Convolutional Layer\n",
        "        layers_rcnn_seq = []\n",
        "        layers_rcnn_seq.append(L.Conv1D(**config['conv_0']))\n",
        "\n",
        "        # Max Pooling\n",
        "        layers_sensor = []\n",
        "        layers_sensor.append(L.GlobalMaxPooling1D())\n",
        "        layer_concat = L.Concatenate(**config['concat'])\n",
        "\n",
        "        # Two fully connected layers\n",
        "        # Dropout to reduce overfitting\n",
        "        layers_full_connect = []\n",
        "        layers_full_connect.append(L.Dropout(**config['dropout']))\n",
        "        layers_full_connect.append(L.Dense(output_dim, **config['activation_layer']))\n",
        "\n",
        "        tensor = embed_model.output\n",
        "        for layer in layers_rcnn_seq:\n",
        "            tensor = layer(tensor)\n",
        "\n",
        "        tensor_output = layers_sensor[0](tensor)\n",
        "\n",
        "        for layer in layers_full_connect:\n",
        "            tensor_output = layer(tensor_output)\n",
        "\n",
        "        self.tf_model = keras.Model(embed_model.inputs, tensor_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Model \n",
        "Xtr, ytr, Xts, yts, bert_embedding = load_data(file_path, bert_path)\n",
        "embed = BareEmbedding(embedding_size=128, processor=kashgari.processors.class_processor, sequence_length=50)\n",
        "tf_board = TensorBoard(log_dir='tf_dir/cnn_model',\n",
        "                       histogram_freq=5,\n",
        "                       update_freq='batch')\n",
        "K.clear_session()\n",
        "model = My_CNN_Model(embed)\n",
        "model.fit(Xtr, ytr, Xts, yts,callbacks=[tf_board],epochs=30,batch_size=32)\n",
        "report = model.evaluate(Xts, yts)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hwagVxXuuc79",
        "outputId": "db3600bf-a840-46c7-e18c-00c66f422f3e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-15 22:10:03,804 [DEBUG] kashgari - ------------------------------------------------\n",
            "2022-05-15 22:10:03,810 [DEBUG] kashgari - Loaded transformer model's vocab\n",
            "2022-05-15 22:10:03,815 [DEBUG] kashgari - config_path       : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/bert_config.json\n",
            "2022-05-15 22:10:03,819 [DEBUG] kashgari - vocab_path      : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/vocab.txt\n",
            "2022-05-15 22:10:03,825 [DEBUG] kashgari - checkpoint_path : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/bert_model.ckpt\n",
            "2022-05-15 22:10:03,827 [DEBUG] kashgari - Top 50 words    : ['[PAD]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]']\n",
            "2022-05-15 22:10:03,831 [DEBUG] kashgari - ------------------------------------------------\n",
            "100%|██████████| 19550/19550 [00:01<00:00, 11609.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Data Summary:\n",
            "Train\t 13096\n",
            "Test\t 6451\n",
            "Label\t 9\n",
            "9\t 1095\n",
            "1\t 6426\n",
            "7\t 1185\n",
            "2\t 3493\n",
            "3\t 3854\n",
            "4\t 1667\n",
            "8\t 226\n",
            "6\t 1129\n",
            "5\t 472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preparing text vocab dict: 100%|██████████| 13096/13096 [00:00<00:00, 99253.20it/s] \n",
            "Preparing text vocab dict: 100%|██████████| 6451/6451 [00:00<00:00, 103124.73it/s]\n",
            "2022-05-15 22:10:05,868 [DEBUG] kashgari - --- Build vocab dict finished, Total: 2429 ---\n",
            "2022-05-15 22:10:05,870 [DEBUG] kashgari - Top-10: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '的', '是', '一', '们', '我', '个']\n",
            "Preparing classification label vocab dict: 100%|██████████| 13096/13096 [00:00<00:00, 718706.81it/s]\n",
            "Preparing classification label vocab dict: 100%|██████████| 6451/6451 [00:00<00:00, 350462.47it/s]\n",
            "Calculating sequence length: 100%|██████████| 13096/13096 [00:00<00:00, 708966.60it/s]\n",
            "Calculating sequence length: 100%|██████████| 6451/6451 [00:00<00:00, 490020.38it/s]\n",
            "2022-05-15 22:10:06,024 [DEBUG] kashgari - Calculated sequence length = 59\n",
            "2022-05-15 22:10:06,085 [DEBUG] kashgari - Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "layer_embedding (Embedding)  (None, None, 128)         310912    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 32)          20512     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 9)                 297       \n",
            "=================================================================\n",
            "Total params: 331,721\n",
            "Trainable params: 331,721\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  2/409 [..............................] - ETA: 50s - loss: 2.2145 - accuracy: 0.1250WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0094s vs `on_train_batch_end` time: 0.2374s). Check your callbacks.\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 1.8217 - accuracy: 0.3456 - val_loss: 1.6025 - val_accuracy: 0.4529\n",
            "Epoch 2/30\n",
            "409/409 [==============================] - 3s 9ms/step - loss: 1.6115 - accuracy: 0.4337 - val_loss: 1.4939 - val_accuracy: 0.4759\n",
            "Epoch 3/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 1.5021 - accuracy: 0.4768 - val_loss: 1.4450 - val_accuracy: 0.4974\n",
            "Epoch 4/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 1.4139 - accuracy: 0.5159 - val_loss: 1.4300 - val_accuracy: 0.5047\n",
            "Epoch 5/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 1.3410 - accuracy: 0.5392 - val_loss: 1.4335 - val_accuracy: 0.5103\n",
            "Epoch 6/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 1.2908 - accuracy: 0.5564 - val_loss: 1.4333 - val_accuracy: 0.5090\n",
            "Epoch 7/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 1.2260 - accuracy: 0.5772 - val_loss: 1.4519 - val_accuracy: 0.5048\n",
            "Epoch 8/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 1.1739 - accuracy: 0.5993 - val_loss: 1.4673 - val_accuracy: 0.5040\n",
            "Epoch 9/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 1.1348 - accuracy: 0.6102 - val_loss: 1.4925 - val_accuracy: 0.5017\n",
            "Epoch 10/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 1.1004 - accuracy: 0.6245 - val_loss: 1.5161 - val_accuracy: 0.5028\n",
            "Epoch 11/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 1.0460 - accuracy: 0.6428 - val_loss: 1.5540 - val_accuracy: 0.5000\n",
            "Epoch 12/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 1.0237 - accuracy: 0.6460 - val_loss: 1.5777 - val_accuracy: 0.4972\n",
            "Epoch 13/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.9992 - accuracy: 0.6595 - val_loss: 1.5925 - val_accuracy: 0.4956\n",
            "Epoch 14/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.9762 - accuracy: 0.6643 - val_loss: 1.6379 - val_accuracy: 0.4883\n",
            "Epoch 15/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.9475 - accuracy: 0.6741 - val_loss: 1.6635 - val_accuracy: 0.4955\n",
            "Epoch 16/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.9249 - accuracy: 0.6809 - val_loss: 1.6984 - val_accuracy: 0.4868\n",
            "Epoch 17/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.9165 - accuracy: 0.6824 - val_loss: 1.7168 - val_accuracy: 0.4877\n",
            "Epoch 18/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.8865 - accuracy: 0.6915 - val_loss: 1.7604 - val_accuracy: 0.4873\n",
            "Epoch 19/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.8739 - accuracy: 0.6982 - val_loss: 1.7856 - val_accuracy: 0.4821\n",
            "Epoch 20/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.8534 - accuracy: 0.7020 - val_loss: 1.8362 - val_accuracy: 0.4873\n",
            "Epoch 21/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.8534 - accuracy: 0.7038 - val_loss: 1.8362 - val_accuracy: 0.4812\n",
            "Epoch 22/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.8393 - accuracy: 0.7070 - val_loss: 1.8826 - val_accuracy: 0.4775\n",
            "Epoch 23/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.8195 - accuracy: 0.7106 - val_loss: 1.9362 - val_accuracy: 0.4782\n",
            "Epoch 24/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.8072 - accuracy: 0.7198 - val_loss: 1.9772 - val_accuracy: 0.4715\n",
            "Epoch 25/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.8037 - accuracy: 0.7247 - val_loss: 1.9912 - val_accuracy: 0.4695\n",
            "Epoch 26/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.7790 - accuracy: 0.7289 - val_loss: 2.0101 - val_accuracy: 0.4793\n",
            "Epoch 27/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.7713 - accuracy: 0.7260 - val_loss: 2.0290 - val_accuracy: 0.4740\n",
            "Epoch 28/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.7742 - accuracy: 0.7277 - val_loss: 2.0711 - val_accuracy: 0.4731\n",
            "Epoch 29/30\n",
            "409/409 [==============================] - 4s 10ms/step - loss: 0.7593 - accuracy: 0.7302 - val_loss: 2.1127 - val_accuracy: 0.4747\n",
            "Epoch 30/30\n",
            "409/409 [==============================] - 3s 8ms/step - loss: 0.7526 - accuracy: 0.7356 - val_loss: 2.1119 - val_accuracy: 0.4691\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-15 22:11:49,924 [WARNING] kashgari - Sequence length is None, will use the max length of the samples, which is 364\n",
            "2022-05-15 22:11:50,002 [DEBUG] kashgari - predict input shape (6451, 364) x: \n",
            "[[  2   8  21 ...   0   0   0]\n",
            " [  2  22  13 ...   0   0   0]\n",
            " [  2   8  20 ...   0   0   0]\n",
            " ...\n",
            " [  2  10  82 ...   0   0   0]\n",
            " [  2  24  25 ...   0   0   0]\n",
            " [  2 123 120 ...   0   0   0]]\n",
            "2022-05-15 22:11:50,498 [DEBUG] kashgari - predict output shape (6451, 9)\n",
            "2022-05-15 22:11:50,509 [DEBUG] kashgari - predict output argmax: [1 6 1 ... 0 0 1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5171    0.6373    0.5709      2134\n",
            "           2     0.3815    0.4565    0.4157      1104\n",
            "           3     0.4496    0.4444    0.4470      1305\n",
            "           4     0.3874    0.2696    0.3179       549\n",
            "           5     0.1795    0.0449    0.0718       156\n",
            "           6     0.2558    0.1226    0.1657       359\n",
            "           7     0.6192    0.5608    0.5885       403\n",
            "           8     0.0000    0.0000    0.0000        72\n",
            "           9     0.6440    0.4363    0.5202       369\n",
            "\n",
            "    accuracy                         0.4697      6451\n",
            "   macro avg     0.3816    0.3303    0.3442      6451\n",
            "weighted avg     0.4544    0.4697    0.4550      6451\n",
            "\n",
            "{'detail': {'1': {'precision': 0.5171102661596958, 'recall': 0.6373008434864105, 'f1-score': 0.5709487825356843, 'support': 2134}, '2': {'precision': 0.38152914458743376, 'recall': 0.45652173913043476, 'f1-score': 0.4156701030927835, 'support': 1104}, '3': {'precision': 0.4496124031007752, 'recall': 0.4444444444444444, 'f1-score': 0.44701348747591524, 'support': 1305}, '4': {'precision': 0.387434554973822, 'recall': 0.26958105646630237, 'f1-score': 0.317937701396348, 'support': 549}, '5': {'precision': 0.1794871794871795, 'recall': 0.04487179487179487, 'f1-score': 0.07179487179487179, 'support': 156}, '6': {'precision': 0.2558139534883721, 'recall': 0.12256267409470752, 'f1-score': 0.1657250470809793, 'support': 359}, '7': {'precision': 0.6191780821917808, 'recall': 0.5607940446650124, 'f1-score': 0.5885416666666666, 'support': 403}, '8': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 72}, '9': {'precision': 0.644, 'recall': 0.4363143631436314, 'f1-score': 0.5201938610662359, 'support': 369}, 'accuracy': 0.46969462098899395, 'macro avg': {'precision': 0.38157395377656217, 'recall': 0.3302656622558598, 'f1-score': 0.34420283567883164, 'support': 6451}, 'weighted avg': {'precision': 0.45437439416556663, 'recall': 0.46969462098899395, 'f1-score': 0.45497349453889735, 'support': 6451}}, 'precision': 0.45437439416556663, 'recall': 0.46969462098899395, 'f1-score': 0.45497349453889735, 'support': 6451}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MY_CNN_BILSTM_Model(ABCClassificationModel):\n",
        "    @classmethod\n",
        "    def default_hyper_parameters(cls) -> Dict[str, Dict[str, Any]]:\n",
        "        return {\n",
        "            'bilstm_0': {\n",
        "                'units': 64,\n",
        "                'return_sequences': True\n",
        "            },\n",
        "            'conv_0': {\n",
        "                'filters': 32,\n",
        "                'kernel_size': 5,\n",
        "                'kernel_initializer': 'normal',\n",
        "                'padding': 'valid',\n",
        "                'activation': 'relu',\n",
        "                'strides': 1\n",
        "            },\n",
        "            'concat': {\n",
        "                'axis': 1\n",
        "            },\n",
        "            'dropout': {\n",
        "                'rate': 0.5\n",
        "            },\n",
        "            'activation_layer': {\n",
        "                'activation': 'softmax'\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def build_model_arc(self):\n",
        "        output_dim = len(self.label_processor.vocab2idx)\n",
        "        config = self.hyper_parameters\n",
        "        embed_model = self.embedding.embed_model\n",
        "\n",
        "        # One 1D Convolutional layer and one BiLSTM layer\n",
        "        layers_rcnn_seq = []\n",
        "        layers_rcnn_seq.append(L.Conv1D(**config['conv_0']))\n",
        "        layers_rcnn_seq.append(L.Bidirectional(L.LSTM(**config['bilstm_0'])))\n",
        "\n",
        "        # Max Pooling\n",
        "        layers_sensor = []\n",
        "        layers_sensor.append(L.GlobalMaxPooling1D())\n",
        "        layer_concat = L.Concatenate(**config['concat'])\n",
        "\n",
        "        # Two fully connected layers\n",
        "        layers_full_connect = []\n",
        "        layers_full_connect.append(L.Dropout(**config['dropout']))\n",
        "        layers_full_connect.append(L.Dense(output_dim, **config['activation_layer']))\n",
        "\n",
        "        tensor = embed_model.output\n",
        "        for layer in layers_rcnn_seq:\n",
        "            tensor = layer(tensor)\n",
        "\n",
        "        tensor_output = layers_sensor[0](tensor)\n",
        "\n",
        "        for layer in layers_full_connect:\n",
        "            tensor_output = layer(tensor_output)\n",
        "\n",
        "        self.tf_model = keras.Model(embed_model.inputs, tensor_output)\n"
      ],
      "metadata": {
        "id": "Cgru-smKJcfL"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Model \n",
        "Xtr, ytr, Xts, yts, bert_embedding = load_data(file_path, bert_path)\n",
        "embed = BareEmbedding(embedding_size=128, processor=kashgari.processors.class_processor, sequence_length=50)\n",
        "tf_board = TensorBoard(log_dir='tf_dir/cnn_bilstm_model',\n",
        "                       histogram_freq=5,\n",
        "                       update_freq='batch')\n",
        "K.clear_session()\n",
        "model = MY_CNN_BILSTM_Model(embed)\n",
        "model.fit(Xtr, ytr, Xts, yts,callbacks=[tf_board],epochs=30,batch_size=32)\n",
        "report = model.evaluate(Xts, yts)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItaywjFhJhi_",
        "outputId": "12483b68-b1c9-4709-a9cb-4d5f5d9d887f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-15 22:15:36,950 [DEBUG] kashgari - ------------------------------------------------\n",
            "2022-05-15 22:15:36,951 [DEBUG] kashgari - Loaded transformer model's vocab\n",
            "2022-05-15 22:15:36,952 [DEBUG] kashgari - config_path       : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/bert_config.json\n",
            "2022-05-15 22:15:36,954 [DEBUG] kashgari - vocab_path      : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/vocab.txt\n",
            "2022-05-15 22:15:36,956 [DEBUG] kashgari - checkpoint_path : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/bert_model.ckpt\n",
            "2022-05-15 22:15:36,958 [DEBUG] kashgari - Top 50 words    : ['[PAD]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]']\n",
            "2022-05-15 22:15:36,960 [DEBUG] kashgari - ------------------------------------------------\n",
            "100%|██████████| 19550/19550 [00:00<00:00, 23341.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Data Summary:\n",
            "Train\t 13096\n",
            "Test\t 6451\n",
            "Label\t 9\n",
            "9\t 1095\n",
            "1\t 6426\n",
            "7\t 1185\n",
            "2\t 3493\n",
            "3\t 3854\n",
            "4\t 1667\n",
            "8\t 226\n",
            "6\t 1129\n",
            "5\t 472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preparing text vocab dict: 100%|██████████| 13096/13096 [00:00<00:00, 176013.17it/s]\n",
            "Preparing text vocab dict: 100%|██████████| 6451/6451 [00:00<00:00, 175209.68it/s]\n",
            "2022-05-15 22:15:38,005 [DEBUG] kashgari - --- Build vocab dict finished, Total: 2429 ---\n",
            "2022-05-15 22:15:38,008 [DEBUG] kashgari - Top-10: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '的', '是', '一', '们', '我', '个']\n",
            "Preparing classification label vocab dict: 100%|██████████| 13096/13096 [00:00<00:00, 1071924.07it/s]\n",
            "Preparing classification label vocab dict: 100%|██████████| 6451/6451 [00:00<00:00, 968516.85it/s]\n",
            "Calculating sequence length: 100%|██████████| 13096/13096 [00:00<00:00, 1354355.73it/s]\n",
            "Calculating sequence length: 100%|██████████| 6451/6451 [00:00<00:00, 1301151.96it/s]\n",
            "2022-05-15 22:15:38,099 [DEBUG] kashgari - Calculated sequence length = 59\n",
            "2022-05-15 22:15:38,573 [DEBUG] kashgari - Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "layer_embedding (Embedding)  (None, None, 128)         310912    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 32)          20512     \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 128)         49664     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 9)                 1161      \n",
            "=================================================================\n",
            "Total params: 382,249\n",
            "Trainable params: 382,249\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  2/409 [..............................] - ETA: 28s - loss: 2.1980 - accuracy: 0.1406WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0207s vs `on_train_batch_end` time: 0.1167s). Check your callbacks.\n",
            "409/409 [==============================] - 8s 20ms/step - loss: 1.7526 - accuracy: 0.3670 - val_loss: 1.5767 - val_accuracy: 0.4524\n",
            "Epoch 2/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 1.4786 - accuracy: 0.4901 - val_loss: 1.4655 - val_accuracy: 0.4877\n",
            "Epoch 3/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 1.3475 - accuracy: 0.5362 - val_loss: 1.4483 - val_accuracy: 0.4927\n",
            "Epoch 4/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 1.2526 - accuracy: 0.5710 - val_loss: 1.4676 - val_accuracy: 0.4869\n",
            "Epoch 5/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 1.1641 - accuracy: 0.6039 - val_loss: 1.4889 - val_accuracy: 0.4950\n",
            "Epoch 6/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 1.0867 - accuracy: 0.6304 - val_loss: 1.5488 - val_accuracy: 0.4930\n",
            "Epoch 7/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 1.0146 - accuracy: 0.6575 - val_loss: 1.6124 - val_accuracy: 0.4876\n",
            "Epoch 8/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.9577 - accuracy: 0.6751 - val_loss: 1.6386 - val_accuracy: 0.4771\n",
            "Epoch 9/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.8980 - accuracy: 0.6920 - val_loss: 1.6890 - val_accuracy: 0.4778\n",
            "Epoch 10/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.8518 - accuracy: 0.7094 - val_loss: 1.7772 - val_accuracy: 0.4711\n",
            "Epoch 11/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.7954 - accuracy: 0.7310 - val_loss: 1.8401 - val_accuracy: 0.4672\n",
            "Epoch 12/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.7552 - accuracy: 0.7385 - val_loss: 1.8809 - val_accuracy: 0.4691\n",
            "Epoch 13/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.7074 - accuracy: 0.7596 - val_loss: 2.0197 - val_accuracy: 0.4602\n",
            "Epoch 14/30\n",
            "409/409 [==============================] - 7s 17ms/step - loss: 0.6697 - accuracy: 0.7729 - val_loss: 2.0805 - val_accuracy: 0.4594\n",
            "Epoch 15/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.6407 - accuracy: 0.7788 - val_loss: 2.0932 - val_accuracy: 0.4513\n",
            "Epoch 16/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.6100 - accuracy: 0.7888 - val_loss: 2.1723 - val_accuracy: 0.4478\n",
            "Epoch 17/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.5705 - accuracy: 0.8039 - val_loss: 2.2379 - val_accuracy: 0.4621\n",
            "Epoch 18/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.5502 - accuracy: 0.8078 - val_loss: 2.3524 - val_accuracy: 0.4608\n",
            "Epoch 19/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.5213 - accuracy: 0.8179 - val_loss: 2.3620 - val_accuracy: 0.4481\n",
            "Epoch 20/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.5072 - accuracy: 0.8236 - val_loss: 2.3933 - val_accuracy: 0.4523\n",
            "Epoch 21/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.4993 - accuracy: 0.8272 - val_loss: 2.4331 - val_accuracy: 0.4568\n",
            "Epoch 22/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.4662 - accuracy: 0.8383 - val_loss: 2.5060 - val_accuracy: 0.4529\n",
            "Epoch 23/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.4471 - accuracy: 0.8422 - val_loss: 2.5303 - val_accuracy: 0.4406\n",
            "Epoch 24/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.4359 - accuracy: 0.8491 - val_loss: 2.6044 - val_accuracy: 0.4549\n",
            "Epoch 25/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.4105 - accuracy: 0.8520 - val_loss: 2.6387 - val_accuracy: 0.4456\n",
            "Epoch 26/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.4061 - accuracy: 0.8552 - val_loss: 2.6711 - val_accuracy: 0.4453\n",
            "Epoch 27/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.3922 - accuracy: 0.8621 - val_loss: 2.6841 - val_accuracy: 0.4487\n",
            "Epoch 28/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.3880 - accuracy: 0.8624 - val_loss: 2.7105 - val_accuracy: 0.4457\n",
            "Epoch 29/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.3721 - accuracy: 0.8641 - val_loss: 2.8538 - val_accuracy: 0.4445\n",
            "Epoch 30/30\n",
            "409/409 [==============================] - 7s 16ms/step - loss: 0.3668 - accuracy: 0.8668 - val_loss: 2.7616 - val_accuracy: 0.4506\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-15 22:19:01,925 [WARNING] kashgari - Sequence length is None, will use the max length of the samples, which is 267\n",
            "2022-05-15 22:19:02,017 [DEBUG] kashgari - predict input shape (6451, 267) x: \n",
            "[[  2  28  27 ...   0   0   0]\n",
            " [  2  22  13 ...   0   0   0]\n",
            " [  2  31 850 ...   0   0   0]\n",
            " ...\n",
            " [  2  38  76 ...   0   0   0]\n",
            " [  2  26  40 ...   0   0   0]\n",
            " [  2  10  82 ...   0   0   0]]\n",
            "2022-05-15 22:19:04,624 [DEBUG] kashgari - predict output shape (6451, 9)\n",
            "2022-05-15 22:19:04,633 [DEBUG] kashgari - predict output argmax: [5 0 4 ... 3 0 0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5381    0.5527    0.5453      2106\n",
            "           2     0.4051    0.4262    0.4154      1166\n",
            "           3     0.4178    0.4408    0.4290      1275\n",
            "           4     0.2878    0.3074    0.2973       514\n",
            "           5     0.2520    0.1963    0.2207       163\n",
            "           6     0.2242    0.1615    0.1878       390\n",
            "           7     0.6050    0.5688    0.5863       385\n",
            "           8     0.0909    0.0405    0.0561        74\n",
            "           9     0.5769    0.5556    0.5660       378\n",
            "\n",
            "    accuracy                         0.4508      6451\n",
            "   macro avg     0.3775    0.3611    0.3671      6451\n",
            "weighted avg     0.4453    0.4508    0.4473      6451\n",
            "\n",
            "{'detail': {'1': {'precision': 0.5381414701803051, 'recall': 0.5527065527065527, 'f1-score': 0.5453267744202389, 'support': 2106}, '2': {'precision': 0.4050529747351263, 'recall': 0.4262435677530017, 'f1-score': 0.4153781863769327, 'support': 1166}, '3': {'precision': 0.4178438661710037, 'recall': 0.4407843137254902, 'f1-score': 0.42900763358778626, 'support': 1275}, '4': {'precision': 0.2877959927140255, 'recall': 0.30739299610894943, 'f1-score': 0.297271872060207, 'support': 514}, '5': {'precision': 0.25196850393700787, 'recall': 0.19631901840490798, 'f1-score': 0.2206896551724138, 'support': 163}, '6': {'precision': 0.22419928825622776, 'recall': 0.16153846153846155, 'f1-score': 0.187779433681073, 'support': 390}, '7': {'precision': 0.6049723756906077, 'recall': 0.5688311688311688, 'f1-score': 0.5863453815261044, 'support': 385}, '8': {'precision': 0.09090909090909091, 'recall': 0.04054054054054054, 'f1-score': 0.056074766355140186, 'support': 74}, '9': {'precision': 0.5769230769230769, 'recall': 0.5555555555555556, 'f1-score': 0.5660377358490566, 'support': 378}, 'accuracy': 0.450782824368315, 'macro avg': {'precision': 0.3775340710573858, 'recall': 0.36110135279606986, 'f1-score': 0.36710127100321693, 'support': 6451}, 'weighted avg': {'precision': 0.44528335504118105, 'recall': 0.450782824368315, 'f1-score': 0.44731557727257737, 'support': 6451}}, 'precision': 0.44528335504118105, 'recall': 0.450782824368315, 'f1-score': 0.44731557727257737, 'support': 6451}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MY_Double_BILSTM(ABCClassificationModel):\n",
        "    @classmethod\n",
        "    def default_hyper_parameters(cls) -> Dict[str, Dict[str, Any]]:\n",
        "        return {\n",
        "            'spatial_dropout': {\n",
        "                'rate': 0.2\n",
        "            },\n",
        "            'bilstm_0': {\n",
        "                'units': 64,\n",
        "                'return_sequences': True\n",
        "            },\n",
        "            'bilstm_1': {\n",
        "                'units': 64,\n",
        "                'return_sequences': True\n",
        "            },\n",
        "            'conv_0': {\n",
        "                'filters': 32,\n",
        "                'kernel_size': 5,\n",
        "                'kernel_initializer': 'normal',\n",
        "                'padding': 'valid',\n",
        "                'activation': 'relu',\n",
        "                'strides': 1\n",
        "            },\n",
        "            'concat': {\n",
        "                'axis': 1\n",
        "            },\n",
        "            'dropout': {\n",
        "                'rate': 0.5\n",
        "            },\n",
        "            'activation_layer': {\n",
        "                'activation': 'softmax'\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def build_model_arc(self):\n",
        "        output_dim = len(self.label_processor.vocab2idx)\n",
        "        config = self.hyper_parameters\n",
        "        embed_model = self.embedding.embed_model\n",
        "\n",
        "        # Two BiLSTM Layers\n",
        "        layers_rcnn_seq = []\n",
        "        layers_rcnn_seq.append(L.Bidirectional(L.LSTM(**config['bilstm_0'])))\n",
        "        layers_rcnn_seq.append(L.Bidirectional(L.LSTM(**config['bilstm_1'])))\n",
        "\n",
        "        # Max Pooling\n",
        "        layers_sensor = []\n",
        "        layers_sensor.append(L.GlobalMaxPooling1D())\n",
        "        # layers_sensor.append(L.GlobalAveragePooling1D())\n",
        "        layer_concat = L.Concatenate(**config['concat'])\n",
        "\n",
        "        # Two fully connected layers\n",
        "        layers_full_connect = []\n",
        "        layers_full_connect.append(L.Dropout(**config['dropout']))\n",
        "        layers_full_connect.append(L.Dense(output_dim, **config['activation_layer']))\n",
        "\n",
        "        tensor = embed_model.output\n",
        "        for layer in layers_rcnn_seq:\n",
        "            tensor = layer(tensor)\n",
        "\n",
        "        tensor_output = layers_sensor[0](tensor)\n",
        "\n",
        "        for layer in layers_full_connect:\n",
        "            tensor_output = layer(tensor_output)\n",
        "\n",
        "        self.tf_model = keras.Model(embed_model.inputs, tensor_output)\n"
      ],
      "metadata": {
        "id": "BvVPYMxtJagg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtr, ytr, Xts, yts, bert_embedding = load_data(file_path, bert_path)\n",
        "embed = BareEmbedding(embedding_size=128, processor=kashgari.processors.class_processor, sequence_length=50)\n",
        "tf_board = TensorBoard(log_dir='tf_dir/double_bilstm_model',\n",
        "                       histogram_freq=5,\n",
        "                       update_freq='batch')\n",
        "K.clear_session()\n",
        "model = MY_Double_BILSTM(embed)\n",
        "model.fit(Xtr, ytr, Xts, yts,callbacks=[tf_board],epochs=30,batch_size=32)\n",
        "report = model.evaluate(Xts, yts)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGLd2JR7LhnH",
        "outputId": "76c712c4-faf9-455e-ff2b-7877cbb35737"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-15 22:26:13,627 [DEBUG] kashgari - ------------------------------------------------\n",
            "2022-05-15 22:26:13,628 [DEBUG] kashgari - Loaded transformer model's vocab\n",
            "2022-05-15 22:26:13,631 [DEBUG] kashgari - config_path       : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/bert_config.json\n",
            "2022-05-15 22:26:13,632 [DEBUG] kashgari - vocab_path      : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/vocab.txt\n",
            "2022-05-15 22:26:13,634 [DEBUG] kashgari - checkpoint_path : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/bert_model.ckpt\n",
            "2022-05-15 22:26:13,641 [DEBUG] kashgari - Top 50 words    : ['[PAD]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]']\n",
            "2022-05-15 22:26:13,642 [DEBUG] kashgari - ------------------------------------------------\n",
            "100%|██████████| 19550/19550 [00:00<00:00, 22982.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Data Summary:\n",
            "Train\t 13096\n",
            "Test\t 6451\n",
            "Label\t 9\n",
            "9\t 1095\n",
            "1\t 6426\n",
            "7\t 1185\n",
            "2\t 3493\n",
            "3\t 3854\n",
            "4\t 1667\n",
            "8\t 226\n",
            "6\t 1129\n",
            "5\t 472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preparing text vocab dict: 100%|██████████| 13096/13096 [00:00<00:00, 172205.64it/s]\n",
            "Preparing text vocab dict: 100%|██████████| 6451/6451 [00:00<00:00, 187850.73it/s]\n",
            "2022-05-15 22:26:14,697 [DEBUG] kashgari - --- Build vocab dict finished, Total: 2429 ---\n",
            "2022-05-15 22:26:14,701 [DEBUG] kashgari - Top-10: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '的', '是', '一', '们', '我', '个']\n",
            "Preparing classification label vocab dict: 100%|██████████| 13096/13096 [00:00<00:00, 1072971.02it/s]\n",
            "Preparing classification label vocab dict: 100%|██████████| 6451/6451 [00:00<00:00, 1211328.97it/s]\n",
            "Calculating sequence length: 100%|██████████| 13096/13096 [00:00<00:00, 1114939.41it/s]\n",
            "Calculating sequence length: 100%|██████████| 6451/6451 [00:00<00:00, 1179590.86it/s]\n",
            "2022-05-15 22:26:14,794 [DEBUG] kashgari - Calculated sequence length = 59\n",
            "2022-05-15 22:26:18,592 [DEBUG] kashgari - Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "layer_embedding (Embedding)  (None, None, 128)         310912    \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, None, 128)         98816     \n",
            "_________________________________________________________________\n",
            "bidirectional_1 (Bidirection (None, None, 128)         98816     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 9)                 1161      \n",
            "=================================================================\n",
            "Total params: 509,705\n",
            "Trainable params: 509,705\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  2/409 [..............................] - ETA: 1:30 - loss: 2.1922 - accuracy: 0.2500WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0431s vs `on_train_batch_end` time: 0.4004s). Check your callbacks.\n",
            "409/409 [==============================] - 19s 46ms/step - loss: 1.6943 - accuracy: 0.3913 - val_loss: 1.4879 - val_accuracy: 0.4715\n",
            "Epoch 2/30\n",
            "409/409 [==============================] - 15s 36ms/step - loss: 1.4242 - accuracy: 0.5051 - val_loss: 1.4562 - val_accuracy: 0.4902\n",
            "Epoch 3/30\n",
            "409/409 [==============================] - 15s 36ms/step - loss: 1.3174 - accuracy: 0.5452 - val_loss: 1.4454 - val_accuracy: 0.4876\n",
            "Epoch 4/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 1.2369 - accuracy: 0.5696 - val_loss: 1.4653 - val_accuracy: 0.4883\n",
            "Epoch 5/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 1.1742 - accuracy: 0.5916 - val_loss: 1.4979 - val_accuracy: 0.4843\n",
            "Epoch 6/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 1.1133 - accuracy: 0.6132 - val_loss: 1.5086 - val_accuracy: 0.4851\n",
            "Epoch 7/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 1.0517 - accuracy: 0.6303 - val_loss: 1.5856 - val_accuracy: 0.4891\n",
            "Epoch 8/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 1.0044 - accuracy: 0.6526 - val_loss: 1.6562 - val_accuracy: 0.4775\n",
            "Epoch 9/30\n",
            "409/409 [==============================] - 15s 36ms/step - loss: 0.9508 - accuracy: 0.6640 - val_loss: 1.7271 - val_accuracy: 0.4790\n",
            "Epoch 10/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.9032 - accuracy: 0.6819 - val_loss: 1.7627 - val_accuracy: 0.4761\n",
            "Epoch 11/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.8632 - accuracy: 0.6966 - val_loss: 1.8075 - val_accuracy: 0.4729\n",
            "Epoch 12/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.8276 - accuracy: 0.7048 - val_loss: 1.8896 - val_accuracy: 0.4723\n",
            "Epoch 13/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.7823 - accuracy: 0.7252 - val_loss: 1.9048 - val_accuracy: 0.4720\n",
            "Epoch 14/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.7460 - accuracy: 0.7328 - val_loss: 2.0394 - val_accuracy: 0.4663\n",
            "Epoch 15/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.7132 - accuracy: 0.7439 - val_loss: 2.1042 - val_accuracy: 0.4619\n",
            "Epoch 16/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.6832 - accuracy: 0.7534 - val_loss: 2.1772 - val_accuracy: 0.4582\n",
            "Epoch 17/30\n",
            "409/409 [==============================] - 14s 34ms/step - loss: 0.6583 - accuracy: 0.7568 - val_loss: 2.2022 - val_accuracy: 0.4558\n",
            "Epoch 18/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.6316 - accuracy: 0.7691 - val_loss: 2.3205 - val_accuracy: 0.4600\n",
            "Epoch 19/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.5981 - accuracy: 0.7766 - val_loss: 2.3762 - val_accuracy: 0.4557\n",
            "Epoch 20/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.5816 - accuracy: 0.7864 - val_loss: 2.4266 - val_accuracy: 0.4571\n",
            "Epoch 21/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.5607 - accuracy: 0.7924 - val_loss: 2.5345 - val_accuracy: 0.4557\n",
            "Epoch 22/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.5428 - accuracy: 0.7947 - val_loss: 2.5528 - val_accuracy: 0.4516\n",
            "Epoch 23/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.5178 - accuracy: 0.8051 - val_loss: 2.5451 - val_accuracy: 0.4492\n",
            "Epoch 24/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.4928 - accuracy: 0.8125 - val_loss: 2.7133 - val_accuracy: 0.4479\n",
            "Epoch 25/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.4724 - accuracy: 0.8201 - val_loss: 2.7185 - val_accuracy: 0.4490\n",
            "Epoch 26/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.4674 - accuracy: 0.8220 - val_loss: 2.7963 - val_accuracy: 0.4375\n",
            "Epoch 27/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.4492 - accuracy: 0.8276 - val_loss: 2.9046 - val_accuracy: 0.4468\n",
            "Epoch 28/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.4378 - accuracy: 0.8334 - val_loss: 2.8986 - val_accuracy: 0.4381\n",
            "Epoch 29/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.4315 - accuracy: 0.8327 - val_loss: 2.9399 - val_accuracy: 0.4417\n",
            "Epoch 30/30\n",
            "409/409 [==============================] - 14s 35ms/step - loss: 0.4152 - accuracy: 0.8401 - val_loss: 2.9215 - val_accuracy: 0.4409\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-15 22:33:46,700 [WARNING] kashgari - Sequence length is None, will use the max length of the samples, which is 364\n",
            "2022-05-15 22:33:46,782 [DEBUG] kashgari - predict input shape (6451, 364) x: \n",
            "[[  2  23  18 ...   0   0   0]\n",
            " [  2  99  35 ...   0   0   0]\n",
            " [  2  23 790 ...   0   0   0]\n",
            " ...\n",
            " [  2  85  34 ...   0   0   0]\n",
            " [  2  22   8 ...   0   0   0]\n",
            " [  2 100  14 ...   0   0   0]]\n",
            "2022-05-15 22:33:52,727 [DEBUG] kashgari - predict output shape (6451, 9)\n",
            "2022-05-15 22:33:52,734 [DEBUG] kashgari - predict output argmax: [4 5 0 ... 3 0 2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5125    0.5532    0.5321      2077\n",
            "           2     0.4262    0.4063    0.4160      1179\n",
            "           3     0.4590    0.4355    0.4469      1325\n",
            "           4     0.2909    0.2903    0.2906       527\n",
            "           5     0.2041    0.1399    0.1660       143\n",
            "           6     0.1598    0.1436    0.1513       376\n",
            "           7     0.5443    0.5801    0.5616       381\n",
            "           8     0.1294    0.1528    0.1401        72\n",
            "           9     0.4933    0.4987    0.4960       371\n",
            "\n",
            "    accuracy                         0.4416      6451\n",
            "   macro avg     0.3577    0.3556    0.3556      6451\n",
            "weighted avg     0.4367    0.4416    0.4386      6451\n",
            "\n",
            "{'detail': {'1': {'precision': 0.5124888492417484, 'recall': 0.5532017332691381, 'f1-score': 0.5320676082426486, 'support': 2077}, '2': {'precision': 0.4261565836298932, 'recall': 0.40627650551314676, 'f1-score': 0.415979157620495, 'support': 1179}, '3': {'precision': 0.4590294351630867, 'recall': 0.4354716981132076, 'f1-score': 0.44694035631293577, 'support': 1325}, '4': {'precision': 0.2908745247148289, 'recall': 0.2903225806451613, 'f1-score': 0.29059829059829057, 'support': 527}, '5': {'precision': 0.20408163265306123, 'recall': 0.13986013986013987, 'f1-score': 0.16597510373443983, 'support': 143}, '6': {'precision': 0.15976331360946747, 'recall': 0.14361702127659576, 'f1-score': 0.1512605042016807, 'support': 376}, '7': {'precision': 0.5443349753694581, 'recall': 0.5800524934383202, 'f1-score': 0.5616264294790342, 'support': 381}, '8': {'precision': 0.12941176470588237, 'recall': 0.1527777777777778, 'f1-score': 0.14012738853503187, 'support': 72}, '9': {'precision': 0.49333333333333335, 'recall': 0.49865229110512127, 'f1-score': 0.4959785522788204, 'support': 371}, 'accuracy': 0.44163695551077353, 'macro avg': {'precision': 0.35771937915786217, 'recall': 0.3555813601109565, 'f1-score': 0.35561704344481965, 'support': 6451}, 'weighted avg': {'precision': 0.4367343740223782, 'recall': 0.44163695551077353, 'f1-score': 0.4386250805037696, 'support': 6451}}, 'precision': 0.4367343740223782, 'recall': 0.44163695551077353, 'f1-score': 0.4386250805037696, 'support': 6451}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class My_Double_CNN(ABCClassificationModel):\n",
        "    @classmethod\n",
        "    def default_hyper_parameters(cls) -> Dict[str, Dict[str, Any]]:\n",
        "        return {\n",
        "            'spatial_dropout': {\n",
        "                'rate': 0.2\n",
        "            },\n",
        "            'bilstm_0': {\n",
        "                'units': 64,\n",
        "                'return_sequences': True\n",
        "            },\n",
        "            'conv_0': {\n",
        "                'filters': 128,\n",
        "                'kernel_size': 5,\n",
        "                'kernel_initializer': 'normal',\n",
        "                'padding': 'valid',\n",
        "                'activation': 'relu',\n",
        "                'strides': 1\n",
        "            },\n",
        "            'conv_1': {\n",
        "                'filters': 32,\n",
        "                'kernel_size': 5,\n",
        "                'kernel_initializer': 'normal',\n",
        "                'padding': 'valid',\n",
        "                'activation': 'relu',\n",
        "                'strides': 1\n",
        "            },\n",
        "            'concat': {\n",
        "                'axis': 1\n",
        "            },\n",
        "            'dropout': {\n",
        "                'rate': 0.5\n",
        "            },\n",
        "            'activation_layer': {\n",
        "                'activation': 'softmax'\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def build_model_arc(self):\n",
        "        output_dim = len(self.label_processor.vocab2idx)\n",
        "        config = self.hyper_parameters\n",
        "        embed_model = self.embedding.embed_model\n",
        "\n",
        "        # Two Convolution Layers\n",
        "        layers_rcnn_seq = []\n",
        "        layers_rcnn_seq.append(L.Conv1D(**config['conv_0']))\n",
        "        layers_rcnn_seq.append(L.Conv1D(**config['conv_1']))\n",
        "\n",
        "        # Max Pooling\n",
        "        layers_sensor = []\n",
        "        layers_sensor.append(L.GlobalMaxPooling1D())\n",
        "        layer_concat = L.Concatenate(**config['concat'])\n",
        "\n",
        "        # Two fully connected layers\n",
        "        layers_full_connect = []\n",
        "        layers_full_connect.append(L.Dropout(**config['dropout']))\n",
        "        layers_full_connect.append(L.Dense(output_dim, **config['activation_layer']))\n",
        "\n",
        "        tensor = embed_model.output\n",
        "        for layer in layers_rcnn_seq:\n",
        "            tensor = layer(tensor)\n",
        "\n",
        "        tensor_output = layers_sensor[0](tensor)\n",
        "\n",
        "        for layer in layers_full_connect:\n",
        "            tensor_output = layer(tensor_output)\n",
        "\n",
        "        self.tf_model = keras.Model(embed_model.inputs, tensor_output)\n"
      ],
      "metadata": {
        "id": "1aDrpkOOLh9o"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtr, ytr, Xts, yts, bert_embedding = load_data(file_path, bert_path)\n",
        "embed = BareEmbedding(embedding_size=128, processor=kashgari.processors.class_processor, sequence_length=50)\n",
        "tf_board = TensorBoard(log_dir='tf_dir/double_cnn_model',\n",
        "                       histogram_freq=5,\n",
        "                       update_freq='batch')\n",
        "K.clear_session()\n",
        "model = My_Double_CNN(embed)\n",
        "model.fit(Xtr, ytr, Xts, yts,callbacks=[tf_board],epochs=30,batch_size=32)\n",
        "report = model.evaluate(Xts, yts)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbE_KpcaYn_I",
        "outputId": "4a6505ac-f6eb-408e-9284-aaf60c4cf556"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-15 22:40:36,819 [DEBUG] kashgari - ------------------------------------------------\n",
            "2022-05-15 22:40:36,822 [DEBUG] kashgari - Loaded transformer model's vocab\n",
            "2022-05-15 22:40:36,824 [DEBUG] kashgari - config_path       : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/bert_config.json\n",
            "2022-05-15 22:40:36,825 [DEBUG] kashgari - vocab_path      : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/vocab.txt\n",
            "2022-05-15 22:40:36,827 [DEBUG] kashgari - checkpoint_path : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/bert_model.ckpt\n",
            "2022-05-15 22:40:36,829 [DEBUG] kashgari - Top 50 words    : ['[PAD]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]']\n",
            "2022-05-15 22:40:36,831 [DEBUG] kashgari - ------------------------------------------------\n",
            "100%|██████████| 19550/19550 [00:02<00:00, 9507.63it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Data Summary:\n",
            "Train\t 13096\n",
            "Test\t 6451\n",
            "Label\t 9\n",
            "9\t 1095\n",
            "1\t 6426\n",
            "7\t 1185\n",
            "2\t 3493\n",
            "3\t 3854\n",
            "4\t 1667\n",
            "8\t 226\n",
            "6\t 1129\n",
            "5\t 472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preparing text vocab dict: 100%|██████████| 13096/13096 [00:00<00:00, 101525.05it/s]\n",
            "Preparing text vocab dict: 100%|██████████| 6451/6451 [00:00<00:00, 97379.42it/s]\n",
            "2022-05-15 22:40:39,231 [DEBUG] kashgari - --- Build vocab dict finished, Total: 2429 ---\n",
            "2022-05-15 22:40:39,232 [DEBUG] kashgari - Top-10: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '的', '是', '一', '们', '我', '个']\n",
            "Preparing classification label vocab dict: 100%|██████████| 13096/13096 [00:00<00:00, 702447.76it/s]\n",
            "Preparing classification label vocab dict: 100%|██████████| 6451/6451 [00:00<00:00, 522223.71it/s]\n",
            "Calculating sequence length: 100%|██████████| 13096/13096 [00:00<00:00, 308858.35it/s]\n",
            "Calculating sequence length: 100%|██████████| 6451/6451 [00:00<00:00, 234405.74it/s]\n",
            "2022-05-15 22:40:39,415 [DEBUG] kashgari - Calculated sequence length = 59\n",
            "2022-05-15 22:40:39,601 [DEBUG] kashgari - Model: \"functional_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input (InputLayer)           [(None, None)]            0         \n",
            "_________________________________________________________________\n",
            "layer_embedding (Embedding)  (None, None, 128)         310912    \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, None, 128)         82048     \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, None, 32)          20512     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 9)                 297       \n",
            "=================================================================\n",
            "Total params: 413,769\n",
            "Trainable params: 413,769\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  2/409 [..............................] - ETA: 1:22 - loss: 2.2395 - accuracy: 0.0156WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0119s vs `on_train_batch_end` time: 0.3965s). Check your callbacks.\n",
            "409/409 [==============================] - 4s 11ms/step - loss: 1.8439 - accuracy: 0.3276 - val_loss: 1.6280 - val_accuracy: 0.4361\n",
            "Epoch 2/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 1.6274 - accuracy: 0.4310 - val_loss: 1.5471 - val_accuracy: 0.4709\n",
            "Epoch 3/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 1.5140 - accuracy: 0.4695 - val_loss: 1.5273 - val_accuracy: 0.4764\n",
            "Epoch 4/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 1.4371 - accuracy: 0.4964 - val_loss: 1.5306 - val_accuracy: 0.4751\n",
            "Epoch 5/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 1.3537 - accuracy: 0.5191 - val_loss: 1.5373 - val_accuracy: 0.4751\n",
            "Epoch 6/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 1.2737 - accuracy: 0.5509 - val_loss: 1.5838 - val_accuracy: 0.4753\n",
            "Epoch 7/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 1.2251 - accuracy: 0.5684 - val_loss: 1.6275 - val_accuracy: 0.4778\n",
            "Epoch 8/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 1.1576 - accuracy: 0.5927 - val_loss: 1.6406 - val_accuracy: 0.4775\n",
            "Epoch 9/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 1.1044 - accuracy: 0.6061 - val_loss: 1.6872 - val_accuracy: 0.4722\n",
            "Epoch 10/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 1.0568 - accuracy: 0.6262 - val_loss: 1.8231 - val_accuracy: 0.4765\n",
            "Epoch 11/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 1.0207 - accuracy: 0.6384 - val_loss: 1.8420 - val_accuracy: 0.4664\n",
            "Epoch 12/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.9801 - accuracy: 0.6515 - val_loss: 1.9782 - val_accuracy: 0.4695\n",
            "Epoch 13/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.9486 - accuracy: 0.6606 - val_loss: 2.0306 - val_accuracy: 0.4715\n",
            "Epoch 14/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.9105 - accuracy: 0.6744 - val_loss: 2.1028 - val_accuracy: 0.4722\n",
            "Epoch 15/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.8757 - accuracy: 0.6871 - val_loss: 2.0218 - val_accuracy: 0.4507\n",
            "Epoch 16/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.8406 - accuracy: 0.6940 - val_loss: 2.2121 - val_accuracy: 0.4604\n",
            "Epoch 17/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.8276 - accuracy: 0.6979 - val_loss: 2.2750 - val_accuracy: 0.4664\n",
            "Epoch 18/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.8112 - accuracy: 0.7074 - val_loss: 2.3614 - val_accuracy: 0.4562\n",
            "Epoch 19/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.7743 - accuracy: 0.7185 - val_loss: 2.4408 - val_accuracy: 0.4492\n",
            "Epoch 20/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.7624 - accuracy: 0.7257 - val_loss: 2.5341 - val_accuracy: 0.4549\n",
            "Epoch 21/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.7318 - accuracy: 0.7311 - val_loss: 2.5812 - val_accuracy: 0.4492\n",
            "Epoch 22/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.7313 - accuracy: 0.7343 - val_loss: 2.6979 - val_accuracy: 0.4509\n",
            "Epoch 23/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.7044 - accuracy: 0.7421 - val_loss: 2.8423 - val_accuracy: 0.4482\n",
            "Epoch 24/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.6955 - accuracy: 0.7544 - val_loss: 2.8754 - val_accuracy: 0.4433\n",
            "Epoch 25/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.6685 - accuracy: 0.7609 - val_loss: 2.8731 - val_accuracy: 0.4492\n",
            "Epoch 26/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.6636 - accuracy: 0.7588 - val_loss: 2.9299 - val_accuracy: 0.4391\n",
            "Epoch 27/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.6481 - accuracy: 0.7667 - val_loss: 2.8847 - val_accuracy: 0.4363\n",
            "Epoch 28/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.6313 - accuracy: 0.7694 - val_loss: 3.1415 - val_accuracy: 0.4436\n",
            "Epoch 29/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.6308 - accuracy: 0.7719 - val_loss: 3.4101 - val_accuracy: 0.4498\n",
            "Epoch 30/30\n",
            "409/409 [==============================] - 4s 9ms/step - loss: 0.6305 - accuracy: 0.7682 - val_loss: 3.1390 - val_accuracy: 0.4380\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-15 22:42:31,315 [WARNING] kashgari - Sequence length is None, will use the max length of the samples, which is 267\n",
            "2022-05-15 22:42:31,398 [DEBUG] kashgari - predict input shape (6451, 267) x: \n",
            "[[  2 402 972 ...   0   0   0]\n",
            " [  2   8   7 ...   0   0   0]\n",
            " [  2 148  15 ...   0   0   0]\n",
            " ...\n",
            " [  2  50  35 ...   0   0   0]\n",
            " [  2 213   4 ...   0   0   0]\n",
            " [  2 141  65 ...   0   0   0]]\n",
            "2022-05-15 22:42:31,906 [DEBUG] kashgari - predict output shape (6451, 9)\n",
            "2022-05-15 22:42:31,912 [DEBUG] kashgari - predict output argmax: [4 2 2 ... 1 1 3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5197    0.5519    0.5353      2129\n",
            "           2     0.3620    0.4700    0.4090      1100\n",
            "           3     0.4167    0.4066    0.4116      1279\n",
            "           4     0.3357    0.2541    0.2893       551\n",
            "           5     0.2174    0.0602    0.0943       166\n",
            "           6     0.1854    0.1584    0.1709       385\n",
            "           7     0.5895    0.5544    0.5714       386\n",
            "           8     0.0847    0.0676    0.0752        74\n",
            "           9     0.6133    0.4829    0.5404       381\n",
            "\n",
            "    accuracy                         0.4381      6451\n",
            "   macro avg     0.3694    0.3340    0.3441      6451\n",
            "weighted avg     0.4337    0.4381    0.4323      6451\n",
            "\n",
            "{'detail': {'1': {'precision': 0.5196815568332597, 'recall': 0.5519023015500235, 'f1-score': 0.5353075170842826, 'support': 2129}, '2': {'precision': 0.36204481792717086, 'recall': 0.47, 'f1-score': 0.40901898734177206, 'support': 1100}, '3': {'precision': 0.4166666666666667, 'recall': 0.4065676309616888, 'f1-score': 0.41155520379897104, 'support': 1279}, '4': {'precision': 0.33573141486810554, 'recall': 0.2540834845735027, 'f1-score': 0.2892561983471075, 'support': 551}, '5': {'precision': 0.21739130434782608, 'recall': 0.060240963855421686, 'f1-score': 0.09433962264150943, 'support': 166}, '6': {'precision': 0.18541033434650456, 'recall': 0.15844155844155844, 'f1-score': 0.17086834733893555, 'support': 385}, '7': {'precision': 0.5895316804407713, 'recall': 0.5544041450777202, 'f1-score': 0.5714285714285714, 'support': 386}, '8': {'precision': 0.0847457627118644, 'recall': 0.06756756756756757, 'f1-score': 0.07518796992481203, 'support': 74}, '9': {'precision': 0.6133333333333333, 'recall': 0.48293963254593175, 'f1-score': 0.5403817914831132, 'support': 381}, 'accuracy': 0.43807161680359635, 'macro avg': {'precision': 0.3693929857195003, 'recall': 0.3340163649526016, 'f1-score': 0.3441493565987861, 'support': 6451}, 'weighted avg': {'precision': 0.43365933356268016, 'recall': 0.43807161680359635, 'f1-score': 0.4323074489042689, 'support': 6451}}, 'precision': 0.43365933356268016, 'recall': 0.43807161680359635, 'f1-score': 0.4323074489042689, 'support': 6451}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "rbforcPLzyR4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "716f3da1-02df-4452-f957-c0b7ad973de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-15 22:48:37,370 [DEBUG] kashgari - ------------------------------------------------\n",
            "2022-05-15 22:48:37,371 [DEBUG] kashgari - Loaded transformer model's vocab\n",
            "2022-05-15 22:48:37,372 [DEBUG] kashgari - config_path       : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/bert_config.json\n",
            "2022-05-15 22:48:37,374 [DEBUG] kashgari - vocab_path      : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/vocab.txt\n",
            "2022-05-15 22:48:37,376 [DEBUG] kashgari - checkpoint_path : /content/drive/MyDrive/ColabNotebooks/roberta_zh/resources/bert_model.ckpt\n",
            "2022-05-15 22:48:37,377 [DEBUG] kashgari - Top 50 words    : ['[PAD]', '[unused1]', '[unused2]', '[unused3]', '[unused4]', '[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', '[unused11]', '[unused12]', '[unused13]', '[unused14]', '[unused15]', '[unused16]', '[unused17]', '[unused18]', '[unused19]', '[unused20]', '[unused21]', '[unused22]', '[unused23]', '[unused24]', '[unused25]', '[unused26]', '[unused27]', '[unused28]', '[unused29]', '[unused30]', '[unused31]', '[unused32]', '[unused33]', '[unused34]', '[unused35]', '[unused36]', '[unused37]', '[unused38]', '[unused39]', '[unused40]', '[unused41]', '[unused42]', '[unused43]', '[unused44]', '[unused45]', '[unused46]', '[unused47]', '[unused48]', '[unused49]']\n",
            "2022-05-15 22:48:37,379 [DEBUG] kashgari - ------------------------------------------------\n",
            "100%|██████████| 19550/19550 [00:00<00:00, 22465.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Project Data Summary:\n",
            "Train\t 13096\n",
            "Test\t 6451\n",
            "Label\t 9\n",
            "9\t 1095\n",
            "1\t 6426\n",
            "7\t 1185\n",
            "2\t 3493\n",
            "3\t 3854\n",
            "4\t 1667\n",
            "8\t 226\n",
            "6\t 1129\n",
            "5\t 472\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preparing text vocab dict: 100%|██████████| 13096/13096 [00:00<00:00, 167620.10it/s]\n",
            "Preparing text vocab dict: 100%|██████████| 6451/6451 [00:00<00:00, 168621.15it/s]\n",
            "2022-05-15 22:48:38,464 [DEBUG] kashgari - --- Build vocab dict finished, Total: 2429 ---\n",
            "2022-05-15 22:48:38,464 [DEBUG] kashgari - Top-10: ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '的', '是', '一', '们', '我', '个']\n",
            "Preparing classification label vocab dict: 100%|██████████| 13096/13096 [00:00<00:00, 1287077.47it/s]\n",
            "Preparing classification label vocab dict: 100%|██████████| 6451/6451 [00:00<00:00, 1152803.68it/s]\n",
            "Calculating sequence length: 100%|██████████| 13096/13096 [00:00<00:00, 515050.64it/s]\n",
            "Calculating sequence length: 100%|██████████| 6451/6451 [00:00<00:00, 814713.65it/s]\n",
            "2022-05-15 22:48:42,420 [DEBUG] kashgari - Calculated sequence length = 59\n",
            "2022-05-15 22:48:45,666 [DEBUG] kashgari - Model: \"functional_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "Input-Token (InputLayer)        [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Input-Segment (InputLayer)      [(None, None)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token (Embedding)     (None, None, 768)    16226304    Input-Token[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Segment (Embedding)   (None, None, 768)    1536        Input-Segment[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Token-Segment (Add)   (None, None, 768)    0           Embedding-Token[0][0]            \n",
            "                                                                 Embedding-Segment[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Position (PositionEmb (None, None, 768)    393216      Embedding-Token-Segment[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Norm (LayerNormalizat (None, None, 768)    1536        Embedding-Position[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "Embedding-Dropout (Dropout)     (None, None, 768)    0           Embedding-Norm[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    2362368     Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "                                                                 Embedding-Dropout[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    0           Embedding-Dropout[0][0]          \n",
            "                                                                 Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward (Feed (None, None, 768)    4722432     Transformer-0-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Dropo (None, None, 768)    0           Transformer-0-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Add ( (None, None, 768)    0           Transformer-0-MultiHeadSelfAttent\n",
            "                                                                 Transformer-0-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-0-FeedForward-Norm  (None, None, 768)    1536        Transformer-0-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-0-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    0           Transformer-0-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward (Feed (None, None, 768)    4722432     Transformer-1-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Dropo (None, None, 768)    0           Transformer-1-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Add ( (None, None, 768)    0           Transformer-1-MultiHeadSelfAttent\n",
            "                                                                 Transformer-1-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-1-FeedForward-Norm  (None, None, 768)    1536        Transformer-1-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-1-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    0           Transformer-1-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward (Feed (None, None, 768)    4722432     Transformer-2-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Dropo (None, None, 768)    0           Transformer-2-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Add ( (None, None, 768)    0           Transformer-2-MultiHeadSelfAttent\n",
            "                                                                 Transformer-2-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-2-FeedForward-Norm  (None, None, 768)    1536        Transformer-2-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-2-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    0           Transformer-2-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward (Feed (None, None, 768)    4722432     Transformer-3-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Dropo (None, None, 768)    0           Transformer-3-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Add ( (None, None, 768)    0           Transformer-3-MultiHeadSelfAttent\n",
            "                                                                 Transformer-3-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-3-FeedForward-Norm  (None, None, 768)    1536        Transformer-3-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-3-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    0           Transformer-3-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward (Feed (None, None, 768)    4722432     Transformer-4-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Dropo (None, None, 768)    0           Transformer-4-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Add ( (None, None, 768)    0           Transformer-4-MultiHeadSelfAttent\n",
            "                                                                 Transformer-4-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-4-FeedForward-Norm  (None, None, 768)    1536        Transformer-4-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-4-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    0           Transformer-4-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward (Feed (None, None, 768)    4722432     Transformer-5-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Dropo (None, None, 768)    0           Transformer-5-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Add ( (None, None, 768)    0           Transformer-5-MultiHeadSelfAttent\n",
            "                                                                 Transformer-5-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-5-FeedForward-Norm  (None, None, 768)    1536        Transformer-5-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-5-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    0           Transformer-5-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward (Feed (None, None, 768)    4722432     Transformer-6-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Dropo (None, None, 768)    0           Transformer-6-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Add ( (None, None, 768)    0           Transformer-6-MultiHeadSelfAttent\n",
            "                                                                 Transformer-6-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-6-FeedForward-Norm  (None, None, 768)    1536        Transformer-6-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-6-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    0           Transformer-6-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward (Feed (None, None, 768)    4722432     Transformer-7-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Dropo (None, None, 768)    0           Transformer-7-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Add ( (None, None, 768)    0           Transformer-7-MultiHeadSelfAttent\n",
            "                                                                 Transformer-7-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-7-FeedForward-Norm  (None, None, 768)    1536        Transformer-7-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-7-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    0           Transformer-7-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward (Feed (None, None, 768)    4722432     Transformer-8-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Dropo (None, None, 768)    0           Transformer-8-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Add ( (None, None, 768)    0           Transformer-8-MultiHeadSelfAttent\n",
            "                                                                 Transformer-8-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-8-FeedForward-Norm  (None, None, 768)    1536        Transformer-8-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    2362368     Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-8-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    0           Transformer-8-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-MultiHeadSelfAtte (None, None, 768)    1536        Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward (Feed (None, None, 768)    4722432     Transformer-9-MultiHeadSelfAttent\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Dropo (None, None, 768)    0           Transformer-9-FeedForward[0][0]  \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Add ( (None, None, 768)    0           Transformer-9-MultiHeadSelfAttent\n",
            "                                                                 Transformer-9-FeedForward-Dropout\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-9-FeedForward-Norm  (None, None, 768)    1536        Transformer-9-FeedForward-Add[0][\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-9-FeedForward-Norm[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    0           Transformer-9-FeedForward-Norm[0]\n",
            "                                                                 Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward (Fee (None, None, 768)    4722432     Transformer-10-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Drop (None, None, 768)    0           Transformer-10-FeedForward[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Add  (None, None, 768)    0           Transformer-10-MultiHeadSelfAtten\n",
            "                                                                 Transformer-10-FeedForward-Dropou\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-10-FeedForward-Norm (None, None, 768)    1536        Transformer-10-FeedForward-Add[0]\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    2362368     Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-10-FeedForward-Norm[0\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    0           Transformer-10-FeedForward-Norm[0\n",
            "                                                                 Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-MultiHeadSelfAtt (None, None, 768)    1536        Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward (Fee (None, None, 768)    4722432     Transformer-11-MultiHeadSelfAtten\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Drop (None, None, 768)    0           Transformer-11-FeedForward[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Add  (None, None, 768)    0           Transformer-11-MultiHeadSelfAtten\n",
            "                                                                 Transformer-11-FeedForward-Dropou\n",
            "__________________________________________________________________________________________________\n",
            "Transformer-11-FeedForward-Norm (None, None, 768)    1536        Transformer-11-FeedForward-Add[0]\n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   (None, None, 128)    426496      Transformer-11-FeedForward-Norm[0\n",
            "__________________________________________________________________________________________________\n",
            "bidirectional_1 (Bidirectional) (None, None, 128)    98816       bidirectional[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_max_pooling1d (GlobalMax (None, 128)          0           bidirectional_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 128)          0           global_max_pooling1d[0][0]       \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 9)            1161        dropout[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 102,203,529\n",
            "Trainable params: 526,473\n",
            "Non-trainable params: 101,677,056\n",
            "__________________________________________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "  2/409 [..............................] - ETA: 3:02 - loss: 2.0991 - accuracy: 0.2031WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.1835s vs `on_train_batch_end` time: 0.7166s). Check your callbacks.\n",
            "409/409 [==============================] - 110s 270ms/step - loss: 1.6306 - accuracy: 0.4205 - val_loss: 1.4492 - val_accuracy: 0.4896\n",
            "Epoch 2/30\n",
            "409/409 [==============================] - 96s 234ms/step - loss: 1.4460 - accuracy: 0.4874 - val_loss: 1.4059 - val_accuracy: 0.5090\n",
            "Epoch 3/30\n",
            "409/409 [==============================] - 95s 233ms/step - loss: 1.3612 - accuracy: 0.5243 - val_loss: 1.3831 - val_accuracy: 0.5191\n",
            "Epoch 4/30\n",
            "409/409 [==============================] - 95s 233ms/step - loss: 1.2889 - accuracy: 0.5454 - val_loss: 1.3589 - val_accuracy: 0.5221\n",
            "Epoch 5/30\n",
            "409/409 [==============================] - 95s 233ms/step - loss: 1.2261 - accuracy: 0.5665 - val_loss: 1.3401 - val_accuracy: 0.5271\n",
            "Epoch 6/30\n",
            "409/409 [==============================] - 102s 250ms/step - loss: 1.1714 - accuracy: 0.5842 - val_loss: 1.3789 - val_accuracy: 0.5106\n",
            "Epoch 7/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 1.1135 - accuracy: 0.6056 - val_loss: 1.3675 - val_accuracy: 0.5224\n",
            "Epoch 8/30\n",
            "409/409 [==============================] - 95s 233ms/step - loss: 1.0599 - accuracy: 0.6271 - val_loss: 1.4064 - val_accuracy: 0.5162\n",
            "Epoch 9/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 1.0099 - accuracy: 0.6405 - val_loss: 1.4322 - val_accuracy: 0.5190\n",
            "Epoch 10/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 0.9437 - accuracy: 0.6663 - val_loss: 1.4644 - val_accuracy: 0.5082\n",
            "Epoch 11/30\n",
            "409/409 [==============================] - 102s 249ms/step - loss: 0.9007 - accuracy: 0.6803 - val_loss: 1.5001 - val_accuracy: 0.5061\n",
            "Epoch 12/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 0.8486 - accuracy: 0.6966 - val_loss: 1.5827 - val_accuracy: 0.5014\n",
            "Epoch 13/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 0.8019 - accuracy: 0.7149 - val_loss: 1.6810 - val_accuracy: 0.5065\n",
            "Epoch 14/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 0.7598 - accuracy: 0.7281 - val_loss: 1.6305 - val_accuracy: 0.5039\n",
            "Epoch 15/30\n",
            "409/409 [==============================] - 95s 233ms/step - loss: 0.7261 - accuracy: 0.7400 - val_loss: 1.6971 - val_accuracy: 0.4946\n",
            "Epoch 16/30\n",
            "409/409 [==============================] - 102s 250ms/step - loss: 0.7139 - accuracy: 0.7456 - val_loss: 1.8119 - val_accuracy: 0.5042\n",
            "Epoch 17/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 0.6770 - accuracy: 0.7538 - val_loss: 1.7906 - val_accuracy: 0.5050\n",
            "Epoch 18/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 0.6540 - accuracy: 0.7697 - val_loss: 1.8910 - val_accuracy: 0.5078\n",
            "Epoch 19/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 0.6319 - accuracy: 0.7720 - val_loss: 1.8885 - val_accuracy: 0.4970\n",
            "Epoch 20/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 0.6172 - accuracy: 0.7776 - val_loss: 1.9355 - val_accuracy: 0.5016\n",
            "Epoch 21/30\n",
            "409/409 [==============================] - 102s 250ms/step - loss: 0.5932 - accuracy: 0.7854 - val_loss: 1.9594 - val_accuracy: 0.4944\n",
            "Epoch 22/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 0.5809 - accuracy: 0.7909 - val_loss: 1.9020 - val_accuracy: 0.4883\n",
            "Epoch 23/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 0.5773 - accuracy: 0.7932 - val_loss: 1.8872 - val_accuracy: 0.5006\n",
            "Epoch 24/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 0.5507 - accuracy: 0.8024 - val_loss: 2.0287 - val_accuracy: 0.4939\n",
            "Epoch 25/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 0.5392 - accuracy: 0.8042 - val_loss: 1.9226 - val_accuracy: 0.4942\n",
            "Epoch 26/30\n",
            "409/409 [==============================] - 102s 249ms/step - loss: 0.5314 - accuracy: 0.8118 - val_loss: 2.0634 - val_accuracy: 0.4936\n",
            "Epoch 27/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 0.5258 - accuracy: 0.8128 - val_loss: 2.0181 - val_accuracy: 0.4827\n",
            "Epoch 28/30\n",
            "409/409 [==============================] - 95s 233ms/step - loss: 0.5019 - accuracy: 0.8158 - val_loss: 2.1550 - val_accuracy: 0.4914\n",
            "Epoch 29/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 0.5010 - accuracy: 0.8205 - val_loss: 2.1015 - val_accuracy: 0.4908\n",
            "Epoch 30/30\n",
            "409/409 [==============================] - 95s 232ms/step - loss: 0.4946 - accuracy: 0.8240 - val_loss: 2.1365 - val_accuracy: 0.4910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-05-15 23:37:34,911 [WARNING] kashgari - Sequence length is None, will use the max length of the samples, which is 364\n",
            "2022-05-15 23:37:34,997 [DEBUG] kashgari - predict input shape (2, 6451, 364) x: \n",
            "(array([[ 101, 3217, 1921, ...,    0,    0,    0],\n",
            "       [ 101, 1071, 2141, ...,    0,    0,    0],\n",
            "       [ 101, 1086, 6435, ...,    0,    0,    0],\n",
            "       ...,\n",
            "       [ 101,  671,  831, ...,    0,    0,    0],\n",
            "       [ 101, 5018,  676, ...,    0,    0,    0],\n",
            "       [ 101, 1765, 4413, ...,    0,    0,    0]], dtype=int32), array([[0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       ...,\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0],\n",
            "       [0, 0, 0, ..., 0, 0, 0]], dtype=int32))\n",
            "2022-05-15 23:40:37,891 [DEBUG] kashgari - predict output shape (6451, 9)\n",
            "2022-05-15 23:40:37,901 [DEBUG] kashgari - predict output argmax: [2 3 3 ... 4 0 2]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5411    0.6329    0.5834      2100\n",
            "           2     0.4450    0.4936    0.4681      1165\n",
            "           3     0.5266    0.4269    0.4715      1300\n",
            "           4     0.3683    0.3612    0.3647       515\n",
            "           5     0.2929    0.1986    0.2367       146\n",
            "           6     0.2626    0.2114    0.2342       369\n",
            "           7     0.6220    0.5050    0.5574       404\n",
            "           8     0.2308    0.1364    0.1714        88\n",
            "           9     0.5978    0.6044    0.6011       364\n",
            "\n",
            "    accuracy                         0.4942      6451\n",
            "   macro avg     0.4319    0.3967    0.4098      6451\n",
            "weighted avg     0.4895    0.4942    0.4885      6451\n",
            "\n",
            "{'detail': {'1': {'precision': 0.5411237785016286, 'recall': 0.6328571428571429, 'f1-score': 0.5834064969271291, 'support': 2100}, '2': {'precision': 0.44504643962848295, 'recall': 0.49356223175965663, 'f1-score': 0.468050468050468, 'support': 1165}, '3': {'precision': 0.5265654648956357, 'recall': 0.4269230769230769, 'f1-score': 0.4715378079864061, 'support': 1300}, '4': {'precision': 0.3683168316831683, 'recall': 0.3611650485436893, 'f1-score': 0.36470588235294116, 'support': 515}, '5': {'precision': 0.29292929292929293, 'recall': 0.19863013698630136, 'f1-score': 0.236734693877551, 'support': 146}, '6': {'precision': 0.26262626262626265, 'recall': 0.21138211382113822, 'f1-score': 0.23423423423423423, 'support': 369}, '7': {'precision': 0.6219512195121951, 'recall': 0.504950495049505, 'f1-score': 0.5573770491803278, 'support': 404}, '8': {'precision': 0.23076923076923078, 'recall': 0.13636363636363635, 'f1-score': 0.1714285714285714, 'support': 88}, '9': {'precision': 0.5978260869565217, 'recall': 0.6043956043956044, 'f1-score': 0.6010928961748634, 'support': 364}, 'accuracy': 0.4941869477600372, 'macro avg': {'precision': 0.4319060675002688, 'recall': 0.3966921651888613, 'f1-score': 0.4098409000236103, 'support': 6451}, 'weighted avg': {'precision': 0.4895239122649701, 'recall': 0.4941869477600372, 'f1-score': 0.4885001817889677, 'support': 6451}}, 'precision': 0.4895239122649701, 'recall': 0.4941869477600372, 'f1-score': 0.4885001817889677, 'support': 6451}\n"
          ]
        }
      ],
      "source": [
        "# Run Double-BiLSTM model with Roberta pre-train model\n",
        "Xtr, ytr, Xts, yts, bert_embedding = load_data(file_path, bert_path)\n",
        "tf_board = TensorBoard(log_dir='tf_dir/double_bilstm_model',\n",
        "                       histogram_freq=5,\n",
        "                       update_freq='batch')\n",
        "K.clear_session()\n",
        "model = MY_Double_BILSTM(bert_embedding)\n",
        "model.fit(Xtr, ytr, Xts, yts,callbacks=[tf_board],epochs=30,batch_size=32)\n",
        "report = model.evaluate(Xts, yts)\n",
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}